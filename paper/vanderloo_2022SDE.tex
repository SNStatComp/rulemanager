% LaTeX template for the UNECE Worksession on Statistical Data Editing 2015, Budapest
% Last changed: 2015-04-10
% Author: Matthias Templ (using an earlier version by Steven Vale and Daniel Kilchmann)
% Revision and modification by Frederic Picard, Statistics Canada
% April 10 2015
% Revision and modification by Daniel Kilchmann, Swiss Federal Statistical Office
% December 7 2016, April 5 2018, October 8 2019, May 18 2022,

% DISCLAIMER: The UNECE document class style and the LaTeX-template
% are provided without any warranty at all and without any commitment
% to maintain or correct the code. In no event are UNECE or the
% author liable for any damages arising from the use of it.

% The uneceart document class style must be used for right numbering etc.
\documentclass[a4paper,11pt]{style/uneceart}

\usepackage{style/UNECE2022}
\usepackage{enumerate}
\usepackage{xcolor}
\definecolor{unece_color}{RGB}{84, 141, 212}

%%% ----------------------------------------------------------------------
%%% ---   please fill-in your personal data:   ---------------------------

%% the title of your contribution in capital letters:
\newcommand{\TITLE}{\textbf{Rule Management} \\}
%% The type of your paper (contributed or invited)
\newcommand{\TYPE}{\textbf{Invited paper}\vspace*{5mm}\\}

%% Topic (i,ii,iii,iv,v,..) 
%\newcommand{\TOPIC}{Theme (i):  \\  }
%\newcommand{\TOPIC}{Theme (ii):\\  }
%\newcommand{\TOPIC}{Theme (iii): ):  \\  }
%\newcommand{\TOPIC}{Theme (iv):  \\  }
%\newcommand{\TOPIC}{Theme (v):  \\  }
%\newcommand{\TOPIC}{Theme (XXX): XXXXXXX(choose on the themes from the ones in the Latex comments above) \\  }

%% No. of the working paper (fill-in as soon you know it)
\newcommand{\WP}{WP. \\   }
%% author:
\newcommand{\AUTHOR}{Mark van der Loo, Edwin de Jonge, Olav ten Bosch }
%% your organisation
\newcommand{\ORGANISATION}{\AUTHOR{} (Statistics Netherlands, The Netherlands)}
\newcommand{\EMAIL}{mpj.vanderloo@cbs.nl}




\begin{document}

\input{UNECE2022cover.tex}





% Introduction
\section{\hspace*{1ex}INTRODUCTION}  %% The extra hspace is only to align the numbering more or less

\paragraph 


% First section
\section{User stories}  
\label{sect:userstories}

As a statistician producing official statistics, I want to
\begin{enumerate}[S1]
\item Create,  read, update, and delete rules so I can fix my current understanding
   of a statistical domain in the form of a formal ruleset.\label{crud}
\item Select a sequence of rules so I can apply them to my data.
\item Determine the order of rule execution so I have full control over
   data processing and validation.
\item Be able to trace the evolution of my rules and rule sets so I can (a) give full account
   of my production runs, and (b) reproduce production runs.
\item Temporarily remove a rule from one or more rule sets so I can handle exceptional and transient
   data circumstances. This temporary removal should be documented.
\item Temporarily update a rule from one or more rulesets so I can handle exceptional and transient
   data circumstances. This temoporary update should be documented.
\end{enumerate}

As a statistical organization, I want to
\begin{enumerate}[O1]
\item Promote reuse of rules.
\item Promote transparency and learning accross production systems, by
      comparing and benchmarking rules and rule sets.
\end{enumerate}




\section{Formal considerations}
\paragraph The most important thing to do in developing any software system is
to design the formal concepts and relations that are necessary to support its
function. Only then can one guarantee that the resulting system can be fully
understood in the sense that all its capabilities and limitations are known;
that one can reason about its behaviour because it derives from a compact set
of formal concepts and relations; that it is extensible because one can build
complex functionality based on the simple underlying concepts and relations.

\paragraph In the current work, we informally work with data tranformation
rules of various types, and data validation rules and certain tables to store
and manage them. In the following subsections we provide formal definitions for
generic data transformation rules, for the data structures to hold them, and
for the basic operations to manipulate them.



\subsection{What are rules?} \label{sect:rules}
\paragraph The premise of many modern official statistical production systems
is that they should be rule-driven. Yet, a formal definitioin of such data
processing rules seems to be lacking.  Here, we generalize the work on data
validation rules first discussed in \citet{loo2015formal} but see
\cite{loo2018statistical, loo2021datavalidation} for extensive discussions.  In
this work, a data set is a finite collection of data points where a \emph{data
point} is defined as a key-value pair $(k,x)$, where $x$ is the value from a
specified domain $D$ and $k$ is a key that fully identifies the value. In other
words, the key represents necessary and sufficient metadata that identifies the
value. To fully identify a value, it was demonstrated that each key should at
least represent the population, the measurement or observation used to obtain
the value, the unit of the population that was measured, and the variable that
was measured.

\paragraph To understand the ful generality of this definition of a data point,
it helps to see an example. Suppose we ask two questions to two employees,
Alice and Bob, of Statistics Netherlands (SN), namely their Age and in what
Division they work: Economic Statistics (ES) or Social Statistics (SS). In this
case there are four data points, identified by the following keys:
\begin{center}
\begin{tabular}{llll}
Population      & measurement   & unit & variable \\
\hline
Employees of SN & Our questionnaire & Alice & Age     \\
Employees of SN & Our questionnaire & Alice & Division  \\
Employees of SN & Our questionnaire & Bob & Age \\
Employees of SN & Our questionnaire & Bob & Division  \\
\end{tabular}
\end{center}
We are quite permissive in our questionnaire, and it is possible that for
example Bob mixes up the answers to the questions by filling in the Age in the
`Division' field, and the Division in the `Age' field. Therefore, the domain $D$
is the union of all possible answers to all possible questions.
$$
D = \{\textrm{SS},\textrm{ES}\}\cup \mathbb{N}\cup\{\textrm{NA}\},
$$
where $\textrm{NA}$ stands for Not Available, to allow for missing values.  A
combination of a key from the above table and a value from $D$ completely
identifies the value.

\paragraph A \emph{data set} is a collection of data points where each key
occurs exactly once. This gives a data set the structure of a function (a fact
that was also noted by \citet{gelsema2012organisation}) $K\to D$, where $K$ is
a finite set of keys. Observe that we do not force any structure on the data
set. It may consist of a single data point, a record, a few records, or a
collection of random variables from random units from random populations. A
data set is therefore not necessarily relational (as in relational databases)
although in practice they often are. Given a finite set of keys $K$, and a
domain $D$, the set of all possible datasets is the space of functions from $K$
to $D$, denoted $D^K$. We are now ready to define the concept of a rule.

\paragraph A \emph{data transformation rule} $r$ is a function that transforms
one data set into another data set. 
\begin{equation}
r: D^K\to D'^{K'}.
\label{eq:ruledef}
\end{equation}
Here, $D$ and $D'$ may or may not be the same, and similarly $K$ and $K'$ may
or may not be the same. This framework is general enough to cover a wide range
of commonly used types of rules and data processing operations, as will be
demonstrated by the next examples.

\paragraph \textbf{Data validation rules.} The above general definition reduces
to the definition of a data validation rule in \cite{loo2021datavalidation} by
choosing $D=\{0,1\}$, setting $K'$ equal to the singleton set $\{1\}$, and
demanding that the function is surjective. Data validation rules are
expressions that evaluate to a boolean that is interpreted as to whether a
dataset satisfies the demand expressed in the rule or not.

\paragraph \textbf{Conditional data cleaning rules.} These are conditional
expressions of the form \texttt{IF} a certain data condition is met
\texttt{THEN} change the value of a certain variable. In these transformations
the key set does not change, but the Domain $D$ may shrink from a wide range of
possible values to a smaller (allowed) range of values. So we set $D'\subseteq
D$ and $K'=K$ in Equation~\eqref{eq:ruledef}.

\paragraph \textbf{Deriving new variables.} This includes creating new
variables from one or more existing variables. Typical applications are change
of units, classification, or deriving data quality indicators.  In this case
metadata changes as a new Also, the domain might change since new variables may
have new value domains.  So deriving variables falls in the most general
category defined by Equation~\ref{eq:ruledef}.  

\paragraph \textbf{Combining sources.} Merging two data sets, based on common
properties is as common as it is important in statistical data processing.
Before defining this in the context of the current framework, let us look at an
example. Suppose we have a two measurements (for example questionnaires in a
panel) and for a certain variable $X$ we want to compare the value of the
previous measurement $\tau'$ with the current measurement $\tau$.  To combine
the sources, we need to check for each data point with key $(U, \tau',u,X)$
($u$ is the population unit) whether a data point with key $(U,\tau,u,X)$
exists.  If so, we create a new data point where we copy the value from $\tau'$
to $\tau$ into a new variable:
$$
((U,\tau',u,X),x')\mapsto ((U,\tau,u,Y),x').
$$
This amounts to a left join where we can now compare $((U,\tau,u,Y),x')$ with
$((U,\tau,u,X),x)$. Similar operations can be defined where we combine data
from different populations $U$ and $U'$ for example. In this case, the metadata
changes because we define a new, merged variable $Y$, but the domains stay the
same as $Y$ only contains copies of an existing variable $X$. Hence, an operation
that combines sources is a function $D^K\to D^{K'}$ with $K\subseteq K'$.

\paragraph \textbf{Aggregation.} This amounts to counting of otherwise
combining multiple data points in to a single data point using arithmetical,
statistical, or classifying operations. A fundamental aggregation is a
function $D^K\to D'^{\{1\}}$.


\paragraph Modeling data as a function from a set of content metadata elements
to a domain of allowed values has the advantage that it allows for elegant and
formal modeling. This is the case for formal database theories
\citet{codd1970relational}, as well as more current work based on category
theory such as in the work of \citet{gelsema2012organisation}, and of
\citet[Chapter 3]{fong2019seven}. The main disadvantage is that it is difficult
to model certain technical issues. In particular the existence of multiple
values for the same data (duplicate records) are not expressible as functions
from content metadata to a value domain. For example, it is technically
perfectly possible to create a dataset where the same person occurs twice, once
with birth year 1978 and once with birth year 1987. The formal solution is to
extend the key set as defined above and by adding technical metadata, such as
the location in the file. For our current discussion on rule management, these
limitations have no consequences.




\subsection{Rule sequences}
\paragraph In general, it matters in which order data transformations are
executed.  Therefore, the main data structe that users encounter are not rule
sets (as they are usually called) but rather \emph{rule sequences}. That is, a
list of rules $L = (r_1,r_2,\ldots, r_n)$ with a predefined order. When
applying a rule sequence to data, we may think of it as a composed function $L
= r_n\circ r_{n-1}\circ\cdots\circ r_1: D^K\to D'^{K'}$, turing one data set
into another dataset. 



\paragraph  The above consideration suggests that a rule management system
consists principally of a set of labeled rule sequences ${L_1,L_2,\ldots,L_k}$,
where each $L_i$ is a version of a rule sequence, index $i$ labeling the
version.  To keep track of changes, updates are only done on the rule set with
the highest index, and any update results in a new rule sequence that is added
to the set. Principal updates include the following basic operations:
\begin{enumerate}
\item Inserting a rule 
\item Removing a rule
\item Replacing (updating) a rule
\item Changing rule order (permutation).
\end{enumerate}
Formally, we only need two operations to express any of these operations.
Concatenating a single rule at the end of a rule sequence and removing a single
rule at the end of a rule sequence are the most basic operations necessary to
express all the above operations. However, from a user perspective the above
operations are the natural activities. User stories S1 and S5 demand that
versions can be annotated. This means that the basic object is a list of
objects $\langle L_i,M_i\rangle$ where $L_i$ is a rule sequence and $M_i$ holds
metadata about the current version. Given a set of versioned and annotated rule
sequences with $k$ the latest version, any allowed operation 
$$
\phi_{k+1}: \{
\langle L_1,M_1\rangle,
\langle L_2,M_2\rangle,\ldots,
\langle L_k,M_k\rangle\}
\mapsto
\{
\langle L_1,M_1\rangle,
\langle L_2,M_2\rangle,\ldots,
\langle L_k,M_k\rangle,
\langle L_{k+1},M_{k+1}\rangle
\}
$$
where the operation $\phi_{k+1}$ is a combination of insertions, removals,
replacements and permutations. It is not hard to convince oneself that these
operations support user stories S1 (CRUD), S2 (select a sequence), S3
(determine execution order), S5 (temporarily remove rules), S6 (temoporarily
update rules). For support of S4, and in particular the tracability of the
evolution of rule sequences, we need to look into more detail what the contents
of the metadata element $M_{k+1}$ should hold.

\paragraph The problem is that the expression `trace the evolution of rules and
rule sets' is not very precise. If we interpret it as the ability to precicely
reconstruct all changes to a rule sequence, then we need to set $M_i= \phi_i$
(plus some user-defined annotations), and in fact we can suffice by working
only with the sequence of transformations $\phi_1,\phi_{2}\ldots\phi_{k}$,
where it is assumed that $L_1=\phi_1 (\:)$ ($\phi_1$ applied to the empty
list).  If we interpret S4 as the ability to qualitatively reconstruct what
changes were applied and why, we can suffice with $M_i$ representing
user-defined descriptions. The actual changes to rule sequences can be
approximated, for example by computing the shortest sequence of insertions,
removals, substitutions and permutations that change $L_k$ into $L_{k+1}$
(Damerau-Levenshtein distance, see \citet{wagner1975extension}). The only extra
demand is that we are able to compare two rules and test if they are equal or
not. In this work we shall assume that we only need the latter, less precise
version of tracability of the evolution of rule sequences. We expect that users
are less interested in the precise technical steps taken to alter a rule
sequence, and more in the qualitative description of it.










\section{The rulemanager package} \label{sect:rulemanager}


\section{Conclusion and outlook}


\subsection{Some Open Questions}

\paragraph From the examples in Section~\ref{sect:rules} it is clear that rules
can in general not be placed in any order. Simple example: it is only possible
to aggregate a derived variable, after the derivation has taken place.  It is
an interesting question to what extent a rule management system, or tools built
upon a rule management system are capable of detecting such issues. We are not
aware of much research in this area.  \citet{scholtus2016generalized} has done
work in the context of a generalized Fellegi-Holt paradigm, based on affine
transformations of numerical data.  \citet[Chapter 9]{loo2018statistical}
derive some general conditions under which conditional data cleaning rules are
idempotent or commutative. However, as rule-based production systems are slowly
becoming the norm and considering the trend of internationalisation of (parts
of) statistical production, amongst others through the exchange of rules
\cite{bosch2022quality} investigating the quality of rule sets becomes a
pressing issue. We principally distinguish the following research questions.

\paragraph \textbf{Redundancy and consistency} 
Rule sets tend to grow organically in organisations. This means that it is
possible that one part of a rule sequence defines data transformations that are
undone further down the sequence. For reasons of understandability and
maintainability it is preferable to avoid such inconsistencies, or at least to
be able to detect them. Similarly, it is possible for rules to occur twice
or have two rules with a different explicit definition perform the same
activity. Again, it is preferable to avoid or detect such situations. 

\paragraph \textbf{Rule minimization and rule counting} It is hard to count
rules. For example the rule $x\mapsto x+1$ applied twice can be expressed as
$x\mapsto 2$. So it depends on the way changes to a data set are implemented
how these changes are counted. The fact that it is hard to count rules makes it
difficult to objectively compare rule sequences. The logical solution that
comes to mind is to find a procedure that reduces any rule set to a minimal set
of canonical instructions. Indeed this reminds of compiling code and it
possible that, by specializing rule sets to activities typical for statistical
data processing, we can learn something from that area of computer science.


\bibliographystyle{plainnat}
\bibliography{vanderloo}



\end{document}


%\paragraph A \emph{rule repository} $R=\{r_1,\ldots,r_m\}$ is a finite set of
%rules.  The contents of a rule repository may be time-dependent as new rules
%can be added and obsolete rules can be removed. The purpose of a rule
%repository is to offer a rich set of rules from which selections can be made
%depending on the purpose. For example, data processing rules may differ between
%running a flash estimate and an updated estimate, although strong overlap is
%likely to exist.

%\paragraph A \emph{rule sequence} $L = (r_1,\ldots,r_n)$ is a finite ordered
%sequence of rules, where rules come from a predefined, unordered repository.
%Rule sequences are applied to data in order, so by function composition we may
%also think of $L(t)$ as a data transformation function defined by the
%composition $r_n\circ\cdots\circ r_1$. Creating a rule sequence consists of
%selecting a subset from a rule repository and placing them in a certain order.
%The number of ways this can be done grows quickly with the number of rules.
%Even if we assume that each rule may only appear once in a rule sequence, for a
%repository of cardinality $m$, there are principally $\sum_{k=0}^m {m\choose
%k}!$ such lists. For the current discussion we will not make the
%assumption that rules are unique: in the case of data transformation
%rules, there may be a certain need for iteration.
%
%In practice we expect that rule sequences will have significant permutational
%symmetry, in the sense that rule order can be (partially) changed in many cases
%without affecting the action of the sequence on data sets. The trivial case is
%two consecutive rules in the rule sequence that pertain to different variables.
%These can be swapped without any consequences regarding the outcome.
%
%\paragraph From the user point of view, a rule sequence is the main conceptual
%structure to work with. Each production run is controlled by one or more of
%such lists, that may be updated as data circumstances evolve over time. From
%the organizational point of view, the rule repository is more interesting. The
%repository $R(t)$ offers the possibility to avoid duplicates and to discover
%common patterns accross rules and rule evolution. In the case of transformation
%rules, it offers the possibility to see whether different choices are made in
%similar data circumstances; in the case of data validation rules it offers the
%possibility to see whether different quality restrictions are defined for the
%same variable. The collection of rule sequences derived from the rule
%repository are also interesting from the point of view of the organisation as
%it allows for benchmarking and comparing production systems.
%
%\paragraph The basic operations on rule repositories are addition and removal
%of a single rule. These can be represented as set union $\cup$ and set
%difference $-$. For example, if $R=\{r_1,r_2,r_3\}$ then $R\cup
%\{r_4\}=\{r_1,r_2,r_3,r_4\}$ and $R-\{r_2\}=\{r_1,r_3\}$.
%
%\paragraph The basic operations on rule sequences are addition or removal of a
%single rule at the end of the list. The first operation (concatenation,
%$\oplus$) adds an element from a rule repository at the end of a list. We
%denote it here as a labeled unary operator:
%$$
%\oplus_{r}(r_1,\ldots,r_n) = (r_1,\ldots,r_n,r),\: r\in R.
%$$
%Deletion $\ominus$ is a unary operator that removes the last element of a list.
%An empty list is left intact. In notation:
%$$
%\ominus (r_1,\ldots,r_{n-1},r_n) = (r_1,\ldots,r_{n-1}),\textrm{ and } \ominus()=().
%$$
%Together these operations allow for inserting any rule into any position.  For
%example, to insert a rule after location $i$, first delete alle elements
%$n,n-1,\ldots, n-i+1$, concatenate the new rule, and re-concatenate all rules
%previously removed. Similarly,  it is possible to delete a rule from any
%position. By extension, one can permute rules using sequences of removals and
%concatenations.
%
%
%For example, in order to remove the jth element from a rule
%sequence, we just permute with consequtive neighbours until it is the last
%element and apply the $\ominus$ operator. Conversely, we can insert a rule at
%any position by concatenating it at the end with $\oplus$ and then use a
%sequence of permutations to move it into the desired position.

%A permutation $\sigma_{ij}$ swaps the position of the ith and jth element: 
%$$
%\sigma_{ij}(r_1,\ldots, r_i,\ldots,r_j,\ldots, r_n)=
%            (r_1,\ldots, r_j,\ldots,r_i,\ldots, r_n).
%$$


