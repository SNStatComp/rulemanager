% LaTeX template for the UNECE Worksession on Statistical Data Editing 2015, Budapest
% Last changed: 2015-04-10
% Author: Matthias Templ (using an earlier version by Steven Vale and Daniel Kilchmann)
% Revision and modification by Frederic Picard, Statistics Canada
% April 10 2015
% Revision and modification by Daniel Kilchmann, Swiss Federal Statistical Office
% December 7 2016, April 5 2018, October 8 2019, May 18 2022,

% DISCLAIMER: The UNECE document class style and the LaTeX-template
% are provided without any warranty at all and without any commitment
% to maintain or correct the code. In no event are UNECE or the
% author liable for any damages arising from the use of it.

% The uneceart document class style must be used for right numbering etc.
\documentclass[a4paper,11pt]{style/uneceart}

\usepackage{style/UNECE2022}
\usepackage{enumerate}
\usepackage{xcolor}
\definecolor{unece_color}{RGB}{84, 141, 212}

%%% ----------------------------------------------------------------------
%%% ---   please fill-in your personal data:   ---------------------------

%% the title of your contribution in capital letters:
\newcommand{\TITLE}{\textbf{Rule Management} \\}
%% The type of your paper (contributed or invited)
\newcommand{\TYPE}{\textbf{Invited paper}\vspace*{5mm}\\}

%% Topic (i,ii,iii,iv,v,..) 
%\newcommand{\TOPIC}{Theme (i):  \\  }
%\newcommand{\TOPIC}{Theme (ii):\\  }
%\newcommand{\TOPIC}{Theme (iii): ):  \\  }
%\newcommand{\TOPIC}{Theme (iv):  \\  }
%\newcommand{\TOPIC}{Theme (v):  \\  }
%\newcommand{\TOPIC}{Theme (XXX): XXXXXXX(choose on the themes from the ones in the Latex comments above) \\  }

%% No. of the working paper (fill-in as soon you know it)
\newcommand{\WP}{WP. \\   }
%% author:
\newcommand{\AUTHOR}{Mark van der Loo, Edwin de Jonge, Olav ten Bosch }
%% your organisation
\newcommand{\ORGANISATION}{\AUTHOR{} (Statistics Netherlands, The Netherlands)}
\newcommand{\EMAIL}{mpj.vanderloo@cbs.nl}




\begin{document}

\input{UNECE2022cover.tex}





% Introduction
\section{\hspace*{1ex}INTRODUCTION}  %% The extra hspace is only to align the numbering more or less

\paragraph foo bar

\cite{loo2021data}

% First section
\section{User stories}  
\label{sect:userstories}

As a statistician producing official statistics, I want to
\begin{enumerate}[S1]
\item Create, Update, and Delete rules so I can fix my current understanding
   of a statistical domain in the form of a formal ruleset.\label{crud}
\item Select a set of rules so I can apply them to my data.
\item Determine the order of rule execution so I have full control over
   data processing and validation.
\item Be able to trace the evolution of my rules and rule sets so I can (a) give full account
   of my production runs, and (b) reproduce production runs.
\item Temporarily remove a rule from one or more rule sets so I can handle exceptional and transient
   data circumstances. This temporary removal should be documented.
\item Temporarily update a rule from one or more rulesets so I can handle exceptional and transient
   data circumstances. This temoporary update should be documented.
\end{enumerate}

As a statistical organization, I want to
\begin{enumerate}[O1]
\item Promote reuse of rules.
\item Promote transparency and learning accross production systems, by
      comparing and benchmarking rules and rule sets.
\end{enumerate}




\section{Formal considerations}
\paragraph The most important thing to do in developing any software system is
to design the formal concepts and relations that are necessary to support its
function. Only then can one guarantee that the resulting system can be fully
understood in the sense that all its capabilities and limitations are known;
that one can reason about its behaviour because it derives from a compact set
of formal concepts and relations; that it is extensible because one can build
complex functionality based on the simple underlying concepts and relations.

\paragraph In the current work, we informally work with data tranformation
rules of various types, and data validation rules and certain tables to store
and manage them. In the following subsections we provide formal definitions for
generic data transformation rules, for the data structures to hold them, and
for the basic operations to manipulate them.



\subsection{What are rules?}
\paragraph The premise of many modern official statistical production systems
is that they should be rule-driven. Yet, a formal definitioin of such data
processing rules seems to be lacking.  Here, we generalize the work on data
validation rules first discussed in \citet{loo2015formal} but see
\cite{loo2018statistical, loo2021datavalidation} for extensive discussions.  In
this work, a data set is a finite collection of data points where a \emph{data
point} is defined as a key-value pair $(k,x)$, where $x$ is the value from a
specified domain $D$ and $k$ is a key that fully identifies the value. In other
words, the key represents necessary and sufficient metadata that identifies the
value. To fully identify a value, it was demonstrated that each key should at
least represent the population, the measurement or observation used to obtain
the value, the unit of the population that was measured, and the variable that
was measured.

\paragraph To understand the ful generality of this definition of a data point,
it helps to see an example. Suppose we ask two questions to two employees,
Alice and Bob, of Statistics Netherlands (SN), namely their Age and in what
Division they work: Economic Statistics (ES) or Social Statistics (SS). In this
case there are four data points, identified by the following keys:
\begin{center}
\begin{tabular}{llll}
Population      & measurement   & unit & variable \\
\hline
Employees of SN & Our questionnaire & Alice & Age     \\
Employees of SN & Our questionnaire & Alice & Division  \\
Employees of SN & Our questionnaire & Bob & Age \\
Employees of SN & Our questionnaire & Bob & Division  \\
\end{tabular}
\end{center}
We are quite permissive in our questionnaire, and it is possible that for
example Bob mixes up the answers to the questions by filling in the Age in the
`Division' field, and the Division in the `Age' field. Therefore, the domain $D$
is the union of all possible answers to all possible questions.
$$
D = \{\textrm{SS},\textrm{ES}\}\cup \mathbb{N}\cup\{\textrm{NA}\},
$$
where $\textrm{NA}$ stands for Not Available, to allow for missing values.  A
combination of a key from the above table and a value from $D$ completely
identifies the value.

\paragraph A \emph{data set} is a collection of data points where each key
occurs exactly once. This gives a data set the structure of a function (a fact
that was also noted by \citet{gelsema2012organisation}) $K\to D$, where $K$ is
a finite set of keys. Observe that we do not force any structure on the data
set. It may consist of a single data point, a record, a few records, or a
collection of random variables from random units from random populations. A
data set is therefore not necessarily relational (as in relational databases)
although in practice they often are. Given a finite set of keys $K$, and a
domain $D$, the set of all possible datasets is the space of functions from $K$
to $D$, denoted $D^K$. We are now ready to define the concept of a rule.

\paragraph A \emph{data transformation rule} $r$ is a function that transforms
one data set into another data set. 
\begin{equation}
r: D^K\to D'^{K'}.
\label{eq:ruledef}
\end{equation}
Here, $D$ and $D'$ may or may not be the same, and similarly $K$ and $K'$ may
or may not be the same. This framework is general enough to cover a wide range
of commonly used types of rules and data processing operations, as will be
demonstrated by the next examples.

\paragraph \textbf{Data validation rules.} The above general definition reduces
to the definition of a data validation rule in \cite{loo2021datavalidation} by
choosing $D=\{0,1\}$, setting $K'$ equal to the singleton set $\{1\}$, and
demanding that the function is surjective. Data validation rules are
expressions that evaluate to a boolean that is interpreted as to whether a
dataset satisfies the demand expressed in the rule or not.

\paragraph \textbf{Conditional data cleaning rules.} These are conditional
expressions of the form \texttt{IF} a certain data condition is met
\texttt{THEN} change the value of a certain variable. In these transformations
the key set does not change, but the Domain $D$ may shrink from a wide range of
possible values to a smaller (allowed) range of values. So we set $D'\subseteq
D$ and $K'=K$ in Equation~\eqref{eq:ruledef}.

\paragraph \textbf{Deriving new variables.} This includes creating new
variables from one or more existing variables. Typical applications are change
of units, classification, or deriving data quality indicators.  In this case
metadata changes as a new Also, the domain might change since new variables may
have new value domains.  So deriving variables falls in the most general
category defined by Equation~\ref{eq:ruledef}.  

\paragraph \textbf{Combining sources.} Merging two data sets, based on common
properties is as common as it is important in statistical data processing.
Before defining this in the context of the current framework, let us look at an
example. Suppose we have a two measurements (for example questionnaires in a
panel) and for a certain variable $X$ we want to compare the value of the
previous measurement $\tau'$ with the current measurement $\tau$.  To combine
the sources, we need to check for each data point with key $(U, \tau',u,X)$
($u$ is the population unit) whether a data point with key $(U,\tau,u,X)$
exists.  If so, we create a new data point where we copy the value from $\tau'$
to $\tau$ into a new variable:
$$
((U,\tau',u,X),x')\mapsto ((U,\tau,u,Y),x').
$$
This amounts to a left join where we can now compare $((U,\tau,u,Y),x')$ with
$((U,\tau,u,X),x)$. Similar operations can be defined where we combine data
from different populations $U$ and $U'$ for example. In this case, the metadata
changes because we define a new, merged variable $Y$, but the domains stay the
same as $Y$ only contains copies of an existing variable $X$. Hence, an operation
that combines sources is a function $D^K\to D^{K'}$ with $K\subseteq K'$.

\paragraph \textbf{Aggregation.} This amounts to counting of otherwise
combining multiple data points in to a single data point using arithmetical,
statistical, or classifying operations. A fundamental aggregation is a
function $D^K\to D'^{\{1\}}$.



\subsection{Data structures}
\paragraph Having established that the definition of
Equation~\eqref{eq:ruledef} covers many common types of rules, we will now
assume that any rule $r$ is of that form and turn to the question of managing
them. Conceptually we work with two data structures: a rule repository and a
rule sequence. We next define basic operations that enable us to represent the
user stories in Section~\ref{sect:userstories}.

\paragraph A \emph{rule repository} $R=\{r_1,\ldots,r_m\}$ is a finite set of
rules.  The contents of a rule repository may be time-dependent as new rules
can be added and obsolete rules can be removed. The purpose of a rule
repository is to offer a rich set of rules from which selections can be made
depending on the purpose. For example, data processing rules may differ between
running a flash estimate and an updated estimate, although strong overlap is
likely to exist.

\paragraph A \emph{rule sequence} $L = (r_1,\ldots,r_n)$ is a finite ordered
sequence of rules, where rules come from a predefined, unordered repository.
Rule sequences are applied to data in order, so by function composition we may
also think of $L(t)$ as a data transformation function defined by the
composition $r_n\circ\cdots\circ r_1$. Creating a rule sequence consists of
selecting a subset from a rule repository and placing them in a certain order.
The number of ways this can be done grows quickly with the number of rules.
Even if we assume that each rule may only appear once in a rule sequence, for a
repository of cardinality $m$, there are principally $\sum_{k=0}^m {m\choose
k}!$ such lists. For the current discussion we will not make the
assumption that rules are unique: in the case of data transformation
rules, there may be a certain need for iteration.

In practice we expect that rule sequences will have significant permutational
symmetry, in the sense that rule order can be (partially) changed in many cases
without affecting the action of the sequence on data sets. The trivial case is
two consecutive rules in the rule sequence that pertain to different variables.
These can be swapped without any consequences regarding the outcome.

\paragraph From the user point of view, a rule sequence is the main conceptual
structure to work with. Each production run is controlled by one or more of
such lists, that may be updated as data circumstances evolve over time. From
the organizational point of view, the rule repository is more interesting. The
repository $R(t)$ offers the possibility to avoid duplicates and to discover
common patterns accross rules and rule evolution. In the case of transformation
rules, it offers the possibility to see whether different choices are made in
similar data circumstances; in the case of data validation rules it offers the
possibility to see whether different quality restrictions are defined for the
same variable. The collection of rule sequences derived from the rule
repository are also interesting from the point of view of the organisation as
it allows for benchmarking and comparing production systems.

\paragraph The basic operations on rule repositories are addition and removal
of a single rule. These can be represented as set union $\cup$ and set
difference $-$. For example, if $R=\{r_1,r_2,r_3\}$ then $R\cup
\{r_4\}=\{r_1,r_2,r_3,r_4\}$ and $R-\{r_2\}=\{r_1,r_3\}$.

\paragraph The basic operations on rule sequences are addition or removal of a
single rule at the end of the list. The first operation (concatenation,
$\oplus$) adds an element from a rule repository at the end of a list. We
denote it here as a labeled unary operator:
$$
\oplus_{r}(r_1,\ldots,r_n) = (r_1,\ldots,r_n,r),\: r\in R.
$$
Deletion $\ominus$ is a unary operator that removes the last element of a list.
An empty list is left intact. In notation:
$$
\ominus (r_1,\ldots,r_{n-1},r_n) = (r_1,\ldots,r_{n-1}),\textrm{ and } \ominus()=().
$$
Together these operations allow for inserting any rule into any position.  For
example, to insert a rule after location $i$, first delete alle elements
$n,n-1,\ldots, n-i+1$, concatenate the new rule, and re-concatenate all rules
previously removed. Similarly,  it is possible to delete a rule from any
position. By extension, one can permute rules using sequences of removals and
concatenations.


For example, in order to remove the jth element from a rule
sequence, we just permute with consequtive neighbours until it is the last
element and apply the $\ominus$ operator. Conversely, we can insert a rule at
any position by concatenating it at the end with $\oplus$ and then use a
sequence of permutations to move it into the desired position.

%A permutation $\sigma_{ij}$ swaps the position of the ith and jth element: 
%$$
%\sigma_{ij}(r_1,\ldots, r_i,\ldots,r_j,\ldots, r_n)=
%            (r_1,\ldots, r_j,\ldots,r_i,\ldots, r_n).
%$$



\subsection{Implementation design} \label{sc:B}



\section{The rulemanager package}



\bibliographystyle{plainnat}
\bibliography{vanderloo}



\end{document}
